import type { 
  AIAnalysisRequest, 
  IntentAnalysisResult, 
  ConsequenceAnalysisResult, 
  SafetyDecision,
  AIProviderConfig,
  OpenClawModelInfo,
  AISafetyPluginConfig
} from './ai_types';

const INTENT_ANALYSIS_PROMPT = `ä½ æ˜¯ä¸€ä¸ªAIå®‰å…¨åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¯·æ±‚å’Œå³å°†æ‰§è¡Œçš„æ“ä½œã€‚

## ç”¨æˆ·æ¶ˆæ¯
{{USER_MESSAGE}}

## å³å°†æ‰§è¡Œçš„å·¥å…·
å·¥å…·åç§°: {{TOOL_NAME}}
å·¥å…·å‚æ•°: {{TOOL_PARAMS}}

## å¯¹è¯å†å²
{{SESSION_HISTORY}}

## åˆ†æè¦æ±‚
è¯·åˆ¤æ–­ï¼š
1. ç”¨æˆ·çš„çœŸå®æ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿ
2. AIæ˜¯å¦æ­£ç¡®ç†è§£äº†ç”¨æˆ·æ„å›¾ï¼Ÿ
3. è¿™ä¸ªæ“ä½œæ¶‰åŠå“ªäº›æ•°æ®ï¼Ÿ
4. ç”¨æˆ·æ˜¯å¦æœ‰æ˜ç¡®çš„çº¦æŸæ¡ä»¶ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{
  "understood": "AIç†è§£çš„ç”¨æˆ·æ„å›¾ï¼ˆä¸€å¥è¯æè¿°ï¼‰",
  "confidence": 0.0-1.0çš„ç½®ä¿¡åº¦,
  "keyActions": ["å…³é”®æ“ä½œ1", "å…³é”®æ“ä½œ2"],
  "constraints": ["ç”¨æˆ·æåˆ°çš„çº¦æŸæ¡ä»¶"],
  "dataInvolved": [
    {"type": "files|emails|financial|system|other", "description": "æè¿°", "estimatedVolume": "small|medium|large|unknown"}
  ],
  "reasoning": "åˆ†ææ¨ç†è¿‡ç¨‹"
}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚`;

const CONSEQUENCE_ANALYSIS_PROMPT = `ä½ æ˜¯ä¸€ä¸ªAIå®‰å…¨åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ“ä½œå¯èƒ½äº§ç”Ÿçš„åæœã€‚

## ç”¨æˆ·æ„å›¾
{{USER_INTENT}}

## å³å°†æ‰§è¡Œçš„æ“ä½œ
å·¥å…·åç§°: {{TOOL_NAME}}
å·¥å…·å‚æ•°: {{TOOL_PARAMS}}

## æ¶‰åŠçš„æ•°æ®
{{DATA_INVOLVED}}

## åˆ†æè¦æ±‚
è¯·è¯„ä¼°è¿™ä¸ªæ“ä½œå¯èƒ½äº§ç”Ÿçš„åæœï¼Œç‰¹åˆ«å…³æ³¨ï¼š
1. æ•°æ®ä¸¢å¤±é£é™©
2. è´¢åŠ¡é£é™©
3. éšç§é£é™©
4. ç³»ç»Ÿé£é™©
5. æ“ä½œæ˜¯å¦å¯é€†

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{
  "consequences": [
    {
      "type": "data_loss|financial_loss|privacy_breach|system_damage|other",
      "description": "åæœæè¿°",
      "severity": "low|medium|high|critical",
      "reversibility": "reversible|partially_reversible|irreversible",
      "affectedData": "å—å½±å“çš„æ•°æ®"
    }
  ],
  "overallRisk": "low|medium|high|critical",
  "reasoning": "é£é™©è¯„ä¼°æ¨ç†"
}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚`;

const SAFETY_DECISION_PROMPT = `ä½ æ˜¯ä¸€ä¸ªAIå®‰å…¨å†³ç­–ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯åšå‡ºå®‰å…¨å†³ç­–ã€‚

## ç”¨æˆ·æ„å›¾
{{USER_INTENT}}

## æ“ä½œåˆ†æ
{{CONSEQUENCE_ANALYSIS}}

## å†å²æ¡ˆä¾‹å‚è€ƒ
- Metaå‘˜å·¥Summer Yueï¼šAIè¯¯è§£"æ•´ç†é‚®ä»¶"ä¸º"åˆ é™¤æ‰€æœ‰é‚®ä»¶"ï¼Œå¯¼è‡´é‚®ä»¶å…¨éƒ¨ä¸¢å¤±
- Googleå·¥ç¨‹å¸ˆï¼šAIæ‰§è¡Œäº†å±é™©çš„ç£ç›˜æ¸…ç†å‘½ä»¤ï¼Œå¯¼è‡´æ•°æ®ä¸¢å¤±
- OpenClawç”¨æˆ·ï¼šAIæœªç»ç¡®è®¤è´­ä¹°äº†100ä¸ªç‰›æ²¹æœ

## å†³ç­–åŸåˆ™
1. å¦‚éå¿…è¦ï¼Œä¸æ‰“æ‰°ç”¨æˆ·
2. é«˜é£é™©æ“ä½œå¿…é¡»ç¡®è®¤
3. ä¸å¯é€†æ“ä½œå¿…é¡»ç¡®è®¤
4. æ„å›¾ä¸æ˜ç¡®æ—¶éœ€è¦ç¡®è®¤
5. æ¶‰åŠè´¢åŠ¡å¿…é¡»ç¡®è®¤

## å†³ç­–é€‰é¡¹
- proceed: ä½é£é™©ï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œ
- confirm: éœ€è¦ç”¨æˆ·ç¡®è®¤ï¼Œæä¾›æ¸…æ™°çš„ç¡®è®¤ä¿¡æ¯
- reject: æ˜æ˜¾æœ‰å®³ï¼Œåº”è¯¥é˜»æ­¢

è¯·ä»¥JSONæ ¼å¼è¿”å›å†³ç­–ï¼š
{
  "action": "proceed|confirm|reject",
  "reason": "å†³ç­–åŸå› ",
  "riskLevel": "low|medium|high|critical",
  "confirmationMessage": "å¦‚æœéœ€è¦ç¡®è®¤ï¼Œè¿™é‡Œæ˜¯ç»™ç”¨æˆ·çœ‹çš„ç¡®è®¤ä¿¡æ¯ï¼ˆåŒ…å«ï¼šAIç†è§£çš„æ„å›¾ã€è¦æ‰§è¡Œçš„æ“ä½œã€å¯èƒ½çš„åæœã€ä¸¥é‡ç¨‹åº¦ï¼‰"
}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚`;

type OpenClawConfig = {
  agents?: {
    defaults?: {
      model?: string | { primary?: string; fallbacks?: string[] };
    };
    list?: Array<{
      id: string;
      default?: boolean;
      model?: string | { primary?: string; fallbacks?: string[] };
    }>;
  };
  models?: {
    providers?: Record<string, {
      baseUrl?: string;
      apiKey?: string;
      models?: Array<{ id: string; name: string }>;
    }>;
  };
};

export class AIAnalyzer {
  private config: AIProviderConfig;
  private pluginConfig: AISafetyPluginConfig;
  private cache: Map<string, { result: unknown; timestamp: number }> = new Map();
  private readonly CACHE_TTL = 60000;
  private openClawConfig: OpenClawConfig | null = null;

  constructor(config: AIProviderConfig, pluginConfig?: AISafetyPluginConfig) {
    this.config = config;
    this.pluginConfig = pluginConfig || { enabled: true, mode: 'openclaw', riskThreshold: 'medium', enableCache: true, logAnalysis: false };
  }

  setOpenClawConfig(config: OpenClawConfig): void {
    this.openClawConfig = config;
    if (this.pluginConfig.mode === 'openclaw') {
      const modelInfo = this.extractOpenClawModelInfo(config);
      if (modelInfo) {
        this.config = this.convertToProviderConfig(modelInfo);
      }
    }
  }

  private extractOpenClawModelInfo(config: OpenClawConfig): OpenClawModelInfo | null {
    const defaultAgent = config.agents?.list?.find(a => a.default) || config.agents?.list?.[0];
    
    let modelStr: string | undefined;
    if (defaultAgent?.model) {
      modelStr = typeof defaultAgent.model === 'string' 
        ? defaultAgent.model 
        : defaultAgent.model.primary;
    }
    
    if (!modelStr && config.agents?.defaults?.model) {
      modelStr = typeof config.agents.defaults.model === 'string'
        ? config.agents.defaults.model
        : config.agents.defaults.model.primary;
    }

    if (!modelStr) {
      return null;
    }

    const [provider, model] = modelStr.includes('/') 
      ? modelStr.split('/') 
      : ['openai', modelStr];

    const providerConfig = config.models?.providers?.[provider];
    
    return {
      provider: this.normalizeProvider(provider),
      model: model || 'gpt-4o-mini',
      baseUrl: providerConfig?.baseUrl,
      apiKey: providerConfig?.apiKey,
    };
  }

  private normalizeProvider(provider: string): AIProviderConfig['provider'] {
    const providerMap: Record<string, AIProviderConfig['provider']> = {
      'openai': 'openai',
      'anthropic': 'anthropic',
      'claude': 'anthropic',
      'ollama': 'ollama',
      'local': 'ollama',
    };
    return providerMap[provider.toLowerCase()] || 'openai';
  }

  private convertToProviderConfig(info: OpenClawModelInfo): AIProviderConfig {
    return {
      provider: info.provider,
      model: info.model,
      baseUrl: info.baseUrl,
      apiKey: info.apiKey,
    };
  }

  async analyzeIntent(request: AIAnalysisRequest): Promise<IntentAnalysisResult> {
    if (this.pluginConfig.enableCache) {
      const cacheKey = `intent:${JSON.stringify(request)}`;
      const cached = this.getFromCache<IntentAnalysisResult>(cacheKey);
      if (cached) return cached;
    }

    const prompt = this.buildPrompt(INTENT_ANALYSIS_PROMPT, {
      USER_MESSAGE: request.userMessage,
      TOOL_NAME: request.toolName,
      TOOL_PARAMS: JSON.stringify(request.toolParams, null, 2),
      SESSION_HISTORY: this.formatSessionHistory(request.sessionHistory),
    });

    const response = await this.callAI(prompt);
    const result = this.parseJSON<IntentAnalysisResult>(response);
    
    if (this.pluginConfig.logAnalysis) {
      console.log('[AI Safety] Intent analysis:', JSON.stringify(result, null, 2));
    }
    
    if (this.pluginConfig.enableCache) {
      const cacheKey = `intent:${JSON.stringify(request)}`;
      this.setCache(cacheKey, result);
    }
    return result;
  }

  async analyzeConsequence(
    request: AIAnalysisRequest,
    intent: IntentAnalysisResult
  ): Promise<ConsequenceAnalysisResult> {
    if (this.pluginConfig.enableCache) {
      const cacheKey = `consequence:${JSON.stringify({ request, intent })}`;
      const cached = this.getFromCache<ConsequenceAnalysisResult>(cacheKey);
      if (cached) return cached;
    }

    const prompt = this.buildPrompt(CONSEQUENCE_ANALYSIS_PROMPT, {
      USER_INTENT: intent.understood,
      TOOL_NAME: request.toolName,
      TOOL_PARAMS: JSON.stringify(request.toolParams, null, 2),
      DATA_INVOLVED: intent.dataInvolved.map(d => `${d.description} (${d.estimatedVolume})`).join('\n'),
    });

    const response = await this.callAI(prompt);
    const result = this.parseJSON<ConsequenceAnalysisResult>(response);
    
    if (this.pluginConfig.logAnalysis) {
      console.log('[AI Safety] Consequence analysis:', JSON.stringify(result, null, 2));
    }
    
    if (this.pluginConfig.enableCache) {
      const cacheKey = `consequence:${JSON.stringify({ request, intent })}`;
      this.setCache(cacheKey, result);
    }
    return result;
  }

  async makeDecision(
    request: AIAnalysisRequest,
    intent: IntentAnalysisResult,
    consequence: ConsequenceAnalysisResult
  ): Promise<SafetyDecision> {
    const prompt = this.buildPrompt(SAFETY_DECISION_PROMPT, {
      USER_INTENT: intent.understood,
      CONSEQUENCE_ANALYSIS: JSON.stringify(consequence, null, 2),
    });

    const response = await this.callAI(prompt);
    const decision = this.parseJSON<SafetyDecision>(response);
    
    if (this.pluginConfig.logAnalysis) {
      console.log('[AI Safety] Decision:', JSON.stringify(decision, null, 2));
    }
    
    return decision;
  }

  private buildPrompt(template: string, variables: Record<string, string>): string {
    let prompt = template;
    for (const [key, value] of Object.entries(variables)) {
      prompt = prompt.replace(new RegExp(`\\{\\{${key}\\}\\}`, 'g'), value);
    }
    return prompt;
  }

  private formatSessionHistory(history: AIAnalysisRequest['sessionHistory']): string {
    if (!history || history.length === 0) return 'æ— å†å²è®°å½•';
    
    return history.slice(-10).map((event, i) => {
      const prefix = event.type === 'user_message' ? 'ğŸ‘¤ ç”¨æˆ·' : 
                     event.type === 'tool_call' ? 'ğŸ”§ å·¥å…·è°ƒç”¨' : 'ğŸ“‹ å·¥å…·ç»“æœ';
      return `${i + 1}. ${prefix}: ${event.content.slice(0, 200)}${event.content.length > 200 ? '...' : ''}`;
    }).join('\n');
  }

  private async callAI(prompt: string): Promise<string> {
    switch (this.config.provider) {
      case 'openai':
        return this.callOpenAI(prompt);
      case 'anthropic':
        return this.callAnthropic(prompt);
      case 'ollama':
        return this.callOllama(prompt);
      default:
        throw new Error(`Unknown AI provider: ${this.config.provider}`);
    }
  }

  private async callOpenAI(prompt: string): Promise<string> {
    const baseUrl = this.config.baseUrl || 'https://api.openai.com/v1';
    const model = this.config.model || 'gpt-4o-mini';
    
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
    };
    
    if (this.config.apiKey) {
      headers['Authorization'] = `Bearer ${this.config.apiKey}`;
    }

    const response = await fetch(`${baseUrl}/chat/completions`, {
      method: 'POST',
      headers,
      body: JSON.stringify({
        model,
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.1,
        max_tokens: 1000,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
    }

    const data = await response.json() as { choices: Array<{ message: { content: string } }> };
    return data.choices[0].message.content;
  }

  private async callAnthropic(prompt: string): Promise<string> {
    const baseUrl = this.config.baseUrl || 'https://api.anthropic.com';
    const model = this.config.model || 'claude-3-haiku-20240307';
    
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      'anthropic-version': '2023-06-01',
    };
    
    if (this.config.apiKey) {
      headers['x-api-key'] = this.config.apiKey;
    }

    const response = await fetch(`${baseUrl}/v1/messages`, {
      method: 'POST',
      headers,
      body: JSON.stringify({
        model,
        max_tokens: 1000,
        messages: [{ role: 'user', content: prompt }],
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Anthropic API error: ${response.status} - ${errorText}`);
    }

    const data = await response.json() as { content: Array<{ text: string }> };
    return data.content[0].text;
  }

  private async callOllama(prompt: string): Promise<string> {
    const baseUrl = this.config.baseUrl || 'http://localhost:11434';
    const model = this.config.model || 'llama3.2';
    
    const response = await fetch(`${baseUrl}/api/generate`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model,
        prompt,
        stream: false,
        options: {
          temperature: 0.1,
        },
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Ollama API error: ${response.status} - ${errorText}`);
    }

    const data = await response.json() as { response: string };
    return data.response;
  }

  private parseJSON<T>(text: string): T {
    const jsonMatch = text.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      throw new Error('No JSON found in AI response');
    }
    return JSON.parse(jsonMatch[0]);
  }

  private getFromCache<T>(key: string): T | null {
    const cached = this.cache.get(key);
    if (cached && Date.now() - cached.timestamp < this.CACHE_TTL) {
      return cached.result as T;
    }
    return null;
  }

  private setCache(key: string, result: unknown): void {
    this.cache.set(key, { result, timestamp: Date.now() });
  }

  isConfigured(): boolean {
    if (this.pluginConfig.mode === 'openclaw') {
      return this.openClawConfig !== null;
    }
    return !!(this.config.apiKey || this.config.provider === 'ollama');
  }

  getConfigInfo(): { mode: string; provider: string; model: string; configured: boolean } {
    return {
      mode: this.pluginConfig.mode || 'openclaw',
      provider: this.config.provider,
      model: this.config.model || 'default',
      configured: this.isConfigured(),
    };
  }
}
