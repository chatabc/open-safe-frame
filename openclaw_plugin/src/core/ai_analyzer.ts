import type { 
  AIAnalysisRequest, 
  IntentAnalysisResult, 
  ConsequenceAnalysisResult, 
  SafetyDecision,
  AIProviderConfig 
} from './ai_types';

const INTENT_ANALYSIS_PROMPT = `ä½ æ˜¯ä¸€ä¸ªAIå®‰å…¨åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¯·æ±‚å’Œå³å°†æ‰§è¡Œçš„æ“ä½œã€‚

## ç”¨æˆ·æ¶ˆæ¯
{{USER_MESSAGE}}

## å³å°†æ‰§è¡Œçš„å·¥å…·
å·¥å…·åç§°: {{TOOL_NAME}}
å·¥å…·å‚æ•°: {{TOOL_PARAMS}}

## å¯¹è¯å†å²
{{SESSION_HISTORY}}

## åˆ†æè¦æ±‚
è¯·åˆ¤æ–­ï¼š
1. ç”¨æˆ·çš„çœŸå®æ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿ
2. AIæ˜¯å¦æ­£ç¡®ç†è§£äº†ç”¨æˆ·æ„å›¾ï¼Ÿ
3. è¿™ä¸ªæ“ä½œæ¶‰åŠå“ªäº›æ•°æ®ï¼Ÿ
4. ç”¨æˆ·æ˜¯å¦æœ‰æ˜ç¡®çš„çº¦æŸæ¡ä»¶ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{
  "understood": "AIç†è§£çš„ç”¨æˆ·æ„å›¾ï¼ˆä¸€å¥è¯æè¿°ï¼‰",
  "confidence": 0.0-1.0çš„ç½®ä¿¡åº¦,
  "keyActions": ["å…³é”®æ“ä½œ1", "å…³é”®æ“ä½œ2"],
  "constraints": ["ç”¨æˆ·æåˆ°çš„çº¦æŸæ¡ä»¶"],
  "dataInvolved": [
    {"type": "files|emails|financial|system|other", "description": "æè¿°", "estimatedVolume": "small|medium|large|unknown"}
  ],
  "reasoning": "åˆ†ææ¨ç†è¿‡ç¨‹"
}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚`;

const CONSEQUENCE_ANALYSIS_PROMPT = `ä½ æ˜¯ä¸€ä¸ªAIå®‰å…¨åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ“ä½œå¯èƒ½äº§ç”Ÿçš„åæœã€‚

## ç”¨æˆ·æ„å›¾
{{USER_INTENT}}

## å³å°†æ‰§è¡Œçš„æ“ä½œ
å·¥å…·åç§°: {{TOOL_NAME}}
å·¥å…·å‚æ•°: {{TOOL_PARAMS}}

## æ¶‰åŠçš„æ•°æ®
{{DATA_INVOLVED}}

## åˆ†æè¦æ±‚
è¯·è¯„ä¼°è¿™ä¸ªæ“ä½œå¯èƒ½äº§ç”Ÿçš„åæœï¼Œç‰¹åˆ«å…³æ³¨ï¼š
1. æ•°æ®ä¸¢å¤±é£é™©
2. è´¢åŠ¡é£é™©
3. éšç§é£é™©
4. ç³»ç»Ÿé£é™©
5. æ“ä½œæ˜¯å¦å¯é€†

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{
  "consequences": [
    {
      "type": "data_loss|financial_loss|privacy_breach|system_damage|other",
      "description": "åæœæè¿°",
      "severity": "low|medium|high|critical",
      "reversibility": "reversible|partially_reversible|irreversible",
      "affectedData": "å—å½±å“çš„æ•°æ®"
    }
  ],
  "overallRisk": "low|medium|high|critical",
  "reasoning": "é£é™©è¯„ä¼°æ¨ç†"
}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚`;

const SAFETY_DECISION_PROMPT = `ä½ æ˜¯ä¸€ä¸ªAIå®‰å…¨å†³ç­–ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯åšå‡ºå®‰å…¨å†³ç­–ã€‚

## ç”¨æˆ·æ„å›¾
{{USER_INTENT}}

## æ“ä½œåˆ†æ
{{CONSEQUENCE_ANALYSIS}}

## å†å²æ¡ˆä¾‹å‚è€ƒ
- Metaå‘˜å·¥Summer Yueï¼šAIè¯¯è§£"æ•´ç†é‚®ä»¶"ä¸º"åˆ é™¤æ‰€æœ‰é‚®ä»¶"ï¼Œå¯¼è‡´é‚®ä»¶å…¨éƒ¨ä¸¢å¤±
- Googleå·¥ç¨‹å¸ˆï¼šAIæ‰§è¡Œäº†å±é™©çš„ç£ç›˜æ¸…ç†å‘½ä»¤ï¼Œå¯¼è‡´æ•°æ®ä¸¢å¤±
- OpenClawç”¨æˆ·ï¼šAIæœªç»ç¡®è®¤è´­ä¹°äº†100ä¸ªç‰›æ²¹æœ

## å†³ç­–åŸåˆ™
1. å¦‚éå¿…è¦ï¼Œä¸æ‰“æ‰°ç”¨æˆ·
2. é«˜é£é™©æ“ä½œå¿…é¡»ç¡®è®¤
3. ä¸å¯é€†æ“ä½œå¿…é¡»ç¡®è®¤
4. æ„å›¾ä¸æ˜ç¡®æ—¶éœ€è¦ç¡®è®¤
5. æ¶‰åŠè´¢åŠ¡å¿…é¡»ç¡®è®¤

## å†³ç­–é€‰é¡¹
- proceed: ä½é£é™©ï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œ
- confirm: éœ€è¦ç”¨æˆ·ç¡®è®¤ï¼Œæä¾›æ¸…æ™°çš„ç¡®è®¤ä¿¡æ¯
- reject: æ˜æ˜¾æœ‰å®³ï¼Œåº”è¯¥é˜»æ­¢

è¯·ä»¥JSONæ ¼å¼è¿”å›å†³ç­–ï¼š
{
  "action": "proceed|confirm|reject",
  "reason": "å†³ç­–åŸå› ",
  "riskLevel": "low|medium|high|critical",
  "confirmationMessage": "å¦‚æœéœ€è¦ç¡®è®¤ï¼Œè¿™é‡Œæ˜¯ç»™ç”¨æˆ·çœ‹çš„ç¡®è®¤ä¿¡æ¯ï¼ˆåŒ…å«ï¼šAIç†è§£çš„æ„å›¾ã€è¦æ‰§è¡Œçš„æ“ä½œã€å¯èƒ½çš„åæœã€ä¸¥é‡ç¨‹åº¦ï¼‰"
}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚`;

export class AIAnalyzer {
  private config: AIProviderConfig;
  private cache: Map<string, { result: unknown; timestamp: number }> = new Map();
  private readonly CACHE_TTL = 60000;

  constructor(config: AIProviderConfig) {
    this.config = config;
  }

  async analyzeIntent(request: AIAnalysisRequest): Promise<IntentAnalysisResult> {
    const cacheKey = `intent:${JSON.stringify(request)}`;
    const cached = this.getFromCache<IntentAnalysisResult>(cacheKey);
    if (cached) return cached;

    const prompt = this.buildPrompt(INTENT_ANALYSIS_PROMPT, {
      USER_MESSAGE: request.userMessage,
      TOOL_NAME: request.toolName,
      TOOL_PARAMS: JSON.stringify(request.toolParams, null, 2),
      SESSION_HISTORY: this.formatSessionHistory(request.sessionHistory),
    });

    const response = await this.callAI(prompt);
    const result = this.parseJSON<IntentAnalysisResult>(response);
    
    this.setCache(cacheKey, result);
    return result;
  }

  async analyzeConsequence(
    request: AIAnalysisRequest,
    intent: IntentAnalysisResult
  ): Promise<ConsequenceAnalysisResult> {
    const cacheKey = `consequence:${JSON.stringify({ request, intent })}`;
    const cached = this.getFromCache<ConsequenceAnalysisResult>(cacheKey);
    if (cached) return cached;

    const prompt = this.buildPrompt(CONSEQUENCE_ANALYSIS_PROMPT, {
      USER_INTENT: intent.understood,
      TOOL_NAME: request.toolName,
      TOOL_PARAMS: JSON.stringify(request.toolParams, null, 2),
      DATA_INVOLVED: intent.dataInvolved.map(d => `${d.description} (${d.estimatedVolume})`).join('\n'),
    });

    const response = await this.callAI(prompt);
    const result = this.parseJSON<ConsequenceAnalysisResult>(response);
    
    this.setCache(cacheKey, result);
    return result;
  }

  async makeDecision(
    request: AIAnalysisRequest,
    intent: IntentAnalysisResult,
    consequence: ConsequenceAnalysisResult
  ): Promise<SafetyDecision> {
    const prompt = this.buildPrompt(SAFETY_DECISION_PROMPT, {
      USER_INTENT: intent.understood,
      CONSEQUENCE_ANALYSIS: JSON.stringify(consequence, null, 2),
    });

    const response = await this.callAI(prompt);
    return this.parseJSON<SafetyDecision>(response);
  }

  private buildPrompt(template: string, variables: Record<string, string>): string {
    let prompt = template;
    for (const [key, value] of Object.entries(variables)) {
      prompt = prompt.replace(new RegExp(`\\{\\{${key}\\}\\}`, 'g'), value);
    }
    return prompt;
  }

  private formatSessionHistory(history: AIAnalysisRequest['sessionHistory']): string {
    if (!history || history.length === 0) return 'æ— å†å²è®°å½•';
    
    return history.slice(-10).map((event, i) => {
      const prefix = event.type === 'user_message' ? 'ğŸ‘¤ ç”¨æˆ·' : 
                     event.type === 'tool_call' ? 'ğŸ”§ å·¥å…·è°ƒç”¨' : 'ğŸ“‹ å·¥å…·ç»“æœ';
      return `${i + 1}. ${prefix}: ${event.content.slice(0, 200)}${event.content.length > 200 ? '...' : ''}`;
    }).join('\n');
  }

  private async callAI(prompt: string): Promise<string> {
    switch (this.config.provider) {
      case 'openai':
        return this.callOpenAI(prompt);
      case 'anthropic':
        return this.callAnthropic(prompt);
      case 'ollama':
        return this.callOllama(prompt);
      case 'openclaw':
        return this.callOpenClaw(prompt);
      default:
        throw new Error(`Unknown AI provider: ${this.config.provider}`);
    }
  }

  private async callOpenAI(prompt: string): Promise<string> {
    const baseUrl = this.config.baseUrl || 'https://api.openai.com/v1';
    const model = this.config.model || 'gpt-4o-mini';
    
    const response = await fetch(`${baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.config.apiKey}`,
      },
      body: JSON.stringify({
        model,
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.1,
        max_tokens: 1000,
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }

    const data = await response.json();
    return data.choices[0].message.content;
  }

  private async callAnthropic(prompt: string): Promise<string> {
    const baseUrl = this.config.baseUrl || 'https://api.anthropic.com';
    const model = this.config.model || 'claude-3-haiku-20240307';
    
    const response = await fetch(`${baseUrl}/v1/messages`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': this.config.apiKey || '',
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model,
        max_tokens: 1000,
        messages: [{ role: 'user', content: prompt }],
      }),
    });

    if (!response.ok) {
      throw new Error(`Anthropic API error: ${response.status}`);
    }

    const data = await response.json();
    return data.content[0].text;
  }

  private async callOllama(prompt: string): Promise<string> {
    const baseUrl = this.config.baseUrl || 'http://localhost:11434';
    const model = this.config.model || 'llama3.2';
    
    const response = await fetch(`${baseUrl}/api/generate`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model,
        prompt,
        stream: false,
        options: {
          temperature: 0.1,
        },
      }),
    });

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.status}`);
    }

    const data = await response.json();
    return data.response;
  }

  private async callOpenClaw(prompt: string): Promise<string> {
    const baseUrl = this.config.baseUrl || 'http://localhost:3000';
    
    const response = await fetch(`${baseUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        message: prompt,
        stream: false,
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenClaw API error: ${response.status}`);
    }

    const data = await response.json();
    return data.response || data.message || data.content;
  }

  private parseJSON<T>(text: string): T {
    const jsonMatch = text.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      throw new Error('No JSON found in AI response');
    }
    return JSON.parse(jsonMatch[0]);
  }

  private getFromCache<T>(key: string): T | null {
    const cached = this.cache.get(key);
    if (cached && Date.now() - cached.timestamp < this.CACHE_TTL) {
      return cached.result as T;
    }
    return null;
  }

  private setCache(key: string, result: unknown): void {
    this.cache.set(key, { result, timestamp: Date.now() });
  }
}
